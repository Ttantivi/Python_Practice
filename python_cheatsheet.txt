Python Cheat sheet

List Slicing:

list[3:5], only prints 3 and 4.
[start: end], [inclusive, exclusive]

list[:5] start to 5
list[5:] 5 to end

Delete from list:
del(list[index])

Assigning new list, and not just reference
x = [1, 2, 3]
y = list(x)
jeffre d'andria

Methods
this is the .stuff in a python object
fam.index("stuff")

Importing packages
import numpy as np
from numpy import array
import matplotlib.pyplot as plt

bmi > 23 will give a list of true and falses
bmi[bmi > 23] give you stuff that's > 23

Attributes
array.shape # notice how there isn't any (), since it is an attribute.


Plots
import matplotlib.pyplot as plt
plt.plot(year, pop)
plt.xlabel('')
plt.ylabel('')
plt.title('')
plt.grid(True)

plt.show()

plt.scatter(x = gdp_cap, y = life_exp, s = np.array(pop) * 2, c = col,
alpha = 0.8)


Dictionaires
world = {"afghanistan":30.55, "albania":2.77}
Key value pairs

---------
# Create the years and durations lists
years = range(2011, 2020)
durations = [103, 101, 99, 100, 100, 95, 95, 96, 93, 90]

# Create a dictionary with the two lists
movie_dict = {'years':years, 'durations':durations}

# Print the dictionary
movie_dict
---------

adding new stuff to dictionary: dict['key'] = value
del(world['key'])

check if something is in there
'value' in dict

Pandas
df = pd.read_csv('path',  index_col = 0 )
pd.DataFrame(dict)
cars.index = row_labels

brics['country'] # this will return a series
brics[['country']] # this will return a one column data frame

loc # label based
iloc # integer position-based

brics.loc[['RU']] # get row based on label
brics.iloc[[1]] # get row based on index

brics.iloc[[:, [0,1]] # get all rows and columns 0 and 1

filter for row then print column
cars.loc[['RU, 'MOR'], ['Country', 'drives_right']]

pandas true/false
df['new_col'] = df['col'] (logical) 'asdf'

pandas get row based on smallest value
df.nsmallest(n, 'col')

subsetting dataframe
brics[brics['area'] > 8]
brics[np.logical_and(brics['area'] > 8), brics['area' < 10]]

Numpy logical
np.logical_and()
np.logical_or()
np.logical_not()

Convert col type
df['col'] = df['col'].astype('float')


if condition:
     expression
elif x > y:
     expression
else:

while condition:
     expression

for i in array:
     print(i)

for index, height in enumerate(fam):
     index # this is an index that keeps count
     height # this is the actual values of the array

# printing for dictionary
for key, value in dict.items():
     key # key of dict
     value # value of dict

# printing over all elements of np array
for val in np.nditer(array):
	print val

-------------------------------
##### loading csv #####
df = pd.read_csv('csvname.csv', parse_dates[''], index_col='')

df.to_csv('name.csv')

##### Creating a dataframe #####
my_dict = {'key1':value1, 'key2':value2}

list of dictionaries = row by row
dictionary of lists = column by column

pd.DataFrame(stuff)

# now for pandas data frame
# iterate over rows
for lab, row in df.iterrows():
     df.loc[lab, 'name_length'] = len(row['column'])

df['name_length] = df['country'].apply(len)
cars['COUNTRY'] = cars['country'].apply(str.upper)

# summary stuff
df.info() # columns names, type, number of missing values.
df.shape # rows and columns
df.desbribe() # summary statistics
df.values # data values of in 2 dimensional numpy arrays
df.columns # colum nnames
df.index # rows

# Sorting
df.sort_values("column_name", ascending=True)
df.sort_values(["column_name 1", "column_name 2"], ascending=[True, True])

# Subsetting columns
df["column_name"]
df[['column 1', 'column 2']]

# subsetting rows
df[df['height'] > 50]
df[df['breed'] == 'labs']
df[ (df['height'] > 50) & (df['breed'] == 'labs')]
df[df['color'].isin(['Black', 'Brown'])]

# creating new columns
df['meters'] = df['cm']/100

##### Data Aggregation #####
df.['col'].mean(), median, mode, min, max, var, std, sum, quantile
df.['col'].agg([pct30, pct40])
df.['col'].cumsum()

##### Counting #####
df.drop_duplicates(subset="name") # dropping dupluicates based on columns. Get unique values in columns
df.drop_duplicates(subset=['name', 'breed']) # dropping duplicate pairs
df_unique['col'].value_counts()
df_unique['col'].value_counts(sort=True)
df_unique['col'].value_counts(normalize=True)


##### Grouped Summary Statistics #####
dogs.groupby('color')['weight_kg'].mean()
dogs.groupby('[color', 'breed'])[['weight_kg', 'height_cm]].agg([min, max, sum])

df.groupby('col').size() # count of each group.

##### Group by to pivot #####
dogs.pivot_table(values ='weight_kg', index='color', aggfunc=[np.mean, np.median]) # index is the group, values is  the aggregation.

dogs.pivot_table(values ='weight_kg', index='color', columns='breed') # values are the weights, index is rows, columns are columns. 
     fill_value=0 # zeroes instead of NaN's
     margins=True # last row and column will contain means of columns or row, not counting the zeroes.

##### Indexing #####
dogs_ind = dogs.set_index('name') # setting a column as the index
dogs_ind.reset_index() # removing an index
dogs_ind.reset_index(drop=True) # gets rid of the column

dogs_in = dogs.set_index(['breed', 'color'])

dogs.sort_index() # ascending order, default
dogs.sort_index(level=['color', breed'], ascending[True, False])

##### Slicing #####
sort the index before slicing
dogs_srt.loc['Chow Chow":"Poodle"] # final value is included
dogs_srt.loc['first index outer':'last index outer', 'inner':'last inner']

dogs_srt.loc[:, 'name':'height_cm'] # slicing columns and keeping all rows.

dogs.iloc[2:5, 1:4] # slicing by row and column numbers

# Subset rows from Pakistan, Lahore to Russia, Moscow
print(temperatures_srt.loc[('Pakistan', 'Lahore'):('Russia', 'Moscow')])

# Filtering for column A, but only keeping column B.
avocados[avocados['type']=='conventional'][['avg_price']]


##### Aggregating pivot tables #####
dogs_height_by_breed_vs_color.mean(axis = 'index') # summary statistic for each column
dogs_height_by_breed_vs_color.mean(axis = 'columns') # summary statistic for each row


##### Matplotlib stuff #####
# histogram
import matplotlib.pyplot as plt
df['col'].hist(alpha = 0.7, bins = x) # create a histogram
plt.show

alpha = opacity

# barplots
do a group by, then take the mean
avg_weight_by_breed.plot(kind='bar', title='asdf')
plt.show

# lineplots
sully.plt(x='date', y='weight' kind = 'line', rot = 45)
plt.show

sully.plt(x='date', y='weight' kind = 'line', rot = 45)
plt.show

# scatter
dog_pack.plt(x='height', y='weight', kind='scatter)
plt.show

plt.legend([])


##### missing values #####
df.isna() # boolean values if it's missing or not.
df.isna().any() # if there's any missing values in a column
df.isna().sum() # number of missing data by column

df.dropna() # dropping rows
df.fillna(0) # fills values with 0

# keep only nas
df[df.isna()]

# drop rows if containing in specific columns.
df.dropna(subset=['Column1', 'Column2'])


-------------------------------
# random numbers stuff
np.random.seed(123)
np.random.rand()
np.random.ranint(0,2) # randomly between 0 or 1
np.random.rand()



----------------------------------------------------------------------------------------------------------------------------
Join Pandas dataframes

##### inner join #####
joined_df = df1.merge(df2, on='key', suffixes=('_suffix1', '_suffix2'))

suffix if table with same name

joined_df = df1.merge(df2, on=['key1', 'key2'], suffixes=('_suffix1', '_suffix2')) # merge on multiple columns in case of dups

# left or right join
df1.merge(df2, on='id', how='left')

df1.merge(df2, how='right', left_on='id1', right_on='id2) # if id names are not the same in tables.

# self join

##### merging on indexs #####

movies.merge(movie_to_genres, left_on='id', left_index=True, right_on='movie_id', right_index=True) # make sure index is true

##### Filtering Join #####
Filter table 1 based on the values of table 2

Semi-joins
 * returns the intersection, similar to an  inner join
 * returns only columns from the left table and not the right
 * no duplicates

merged_df = df1.merge(df2, on='id')
df1[df1['id'].isin(merged_df['id'])]

Anti-join
 * opposite of semi join

merged_df = df1.merge(df2, on='gid', how='left', indicator=True) # indicator _merge gives (both, left_only, right_only) or source of each row.
id_list = merged_df.loc[merged_df['_merge'] == 'left_only', 'gid']
df1[df1['id'].isin(id_list)]

##### Concatonate Dataframes Vertically #####
pd.concat([df1, df2, df3], ignore_index = True, sort = True) # sorting column names

pd.concat([df1, df2], join='inner') # same columns

# Group the invoices by the index keys and find avg of the total column
avg_inv_by_month = inv_jul_thr_sep.groupby(level=0).agg({'total':'mean'})


##### verifying integrity #####
df1.merge(df2, on='id', validate='one_to_one')

pd.concat([df1,df2], verify_integrity=True) # this will throw an error when there's duplicate rows.

##### pd.merge_ordered() #####
For time series data
pd.merge_ordered(df1, df2, on='date', suffixes=['_df1', '_df2'])

If there's missing data, we can do forward fill. Fill missing row with previous value.
pd.merge_ordered(df1, df2, on='date', suffixes=['_df1', '_df2'], fill_method='ffill')

left_on, right_on when diff id

##### pd.merge_asof() #####
 * match on the nearest key column and not exact matches.
 * ! merged on columns must be sorted !! 

pd.merge_asof(df1, df2, on='date_time', suffixes=('_df1', '_df2'))

##### .query() #####
df1.query('col1 > 90 & col2 < 140')

df1.query('col1 == "hello" ')

##### .melt() #####
unpivots dataset
 * column names bcome row values.
 * adn the values those columns, then now correspond to the rows of the unpivoted columns.

df.melt(id_vars=[], value_vars=['col1', 'col2'], var_name['name variable column (old col)'], value_name=['name of values of unpivoted col'])

value_vars # controls which columns are melted

----------------------------------------------------------------------------------------------------------------------------
Python Stats Stuff
.agg([np.mean,  np.median])

np.std()
np.var()
np.quantile(col, 0.5) # where 0.5 is the quantile.
plt.boxplot(col)
np.linspace(start, stop, num) # num = intervals

iqr = 75th - 25th quartile
from scipy.stats import iqr
iqr(col)

col.describe # count, mean, std, min, 25 50 75 quarts, max

outliers. Less than Q1 - 1.5*IQR
             Greater than Q3 + 1.5*IQR

np.random.seed()

df.sample(n=x, replace =True)


scipy stuff
from scipy.stats import uniform, binom, norm, poisson, expon, 

np.round(uniform.rvs(loc=0, scale=10, size=1000)) # sample from 0 to 10
uniform.cdf(7,0,12) # sample of 7 from 0 to 12

binom.rvs(# of coins, p, size = n)
binom.pmf(value, n, p) # gets the probability of value
binom.cdf(value, n, p) # gets the probability of 0 to value.

norm.rvs(loc=mean, scale=std_dev, size=10)
norm.cdf(value, mean, sd)
norm.ppf(percentage, mean, sd) # what actual say value is at the percentage

poisson.rvs(8, size=10) # lambda = 8, sample size 10
poisson.pmf(5, 8) # where 8 is lambda. Get prob of 5 
poisson.cdf(5, 8) # lambda = 8, prob less that 5.

expon.cdf(value, scale) # where scale is lambda


import seaborn as sns
sns.lmplot(x=, y=, data=, ci = None)
sns.scatterplot(x, y, data)
plt.show()


correlations:
df['x'].corr(df['y'])

transformations
np.log(df['col'])
sqrt()


----------------------------------------------------------------------------------------
Matplot stuff

import matplotlib.pyplot as plt
fig, ax = plt.subplots()

fig is container that holds stuff on page
ax is what holds the data

ax.plot(x, y)
plt.show()

Can add multiple plots on one by calling ax again.


syntax:
ax.plot(x,y,marker='o', linestyle='--', color=r) # marker is marker at points. Can even do none.
ax.set_xlabel('Time')
ax.set_ylabel('Values')
ax.set_title('Title')
ax.tick_params('y', colors='blue')

##### grid of plots #####
fig, ax = plt.subplots(row,columns, sharey=True) # row by column of plots

now ax is an array of objects

ax[0,0].plot(...)

##### for time series #####
ax.plot(df.index, df['col'])

where index is a datetime datatype


##### twin axes #####
ax2 = ax.twinx() # meaing same x axis, but separate y-axes
ax2.plot(...)
ax2.set_ylabel('asdf', color='')
ax.tick_params('y', colors='red')

##### Annotations #####
adding arrows in plot

ax.annotate('annotation', xy = (xposit,yposit), xytext(), arrowprops={'arrowstyle':'->', 'color':'gray'})
# xytext is the position of the text
# arrowprops={} dictionary of arrow properties


##### plots other than line #####

for a stacked barchart
fig, ax = plt.subplots()
ax.bar(df.index, df['col1'], label='col1')
ax.bar(df.index, df['col2'], bottom=df['col1'], label='col2') # do this if want stacked bar chart
ax.bar(df.index, df['col3'], bottom=df['col1'] + df['col2'] + df['col3'], label='col3')

ax.set_xticklabels(df.index, rotation=90)
ax.set_ylabel('stuff')
ax.legend()
plt.show()

histograms, and stacked histograms
fig, ax = plt.subplots()
ax.hist(df['col1'], bins = ) # bins can be list of intervals
ax.hist(df['col2'], bins = ) # can also have histtype='step' where it's just outlines
ax.set_xlabel('')
ax.set_ylabel('')
ax.legend()
plt.show()

Error bars in plots
fig, ax = plt.subplots()
ax.bar('label', df['col'].mean(), yerr=df['col'].std())
ax.errorbar(df['x'], df['y'], yerr=df['y'])


Boxplots
fig, ax = plt.subplots()
ax.boxplot([df['col1'], df['col2']])
ax.set_xticklabels['col1lab','col2lab']

Scatterplots
fig, ax = plt.subplots()
ax.scatter(df1['col1'], df1['col2'], color='red', label='')
ax.scatter(df2['col1'], df2['col2'], color='blue', label='')
ax.legend()
ax.set_xlabel('asdf')
ax.set_ylabel('asdf')

ax.scatter(df1['col1'], df1['col2'], c=df.index, label='') # do this for gradient effect


##### changing plot styles #####
# applies to all figures in section
plt.style.use('ggplot')
plt.style.use('fivethirtyeight')
plt.style.use('default')
plt.style.use('seaborn-colorblind')
plt.style.use('tableau-colorblind10')
plt.style.use('grayscale')

##### saving visualizations #####
fig.savefig('filename.png')
fig.savefig('filename.jpg')
fig.savefig('filename.jpg', quality = x)
fig.savefig('filename.svg', quality = x) # vector file
fig.savefig('filename.svg', dpi = x) # vector file

size of figure
fig.set_size_inches([5,5]) # size and aspect ratio

----------------------------------------------------------------------------------------
#### Seaborn stuff ####
import seaborn as sns
import matplotlib.pyplot as plt

sns.scatterplot(x=df['col'], y=df['col'])
plt.show()

sns.countplot(x=df['col'])

when using dataframes,
sns.countplot(x='col1', data=df)

if color code points, legend is added automatically.
sns.scatterplot(x, y, data=df, hue='col', hue_order=['Yes', 'No'])
sns.countplot(x, data=df, hue='col')
plt.show()

Or to be more specific with the colors
hue_colors = {'Yes':'black', 'No':'red'}
sns.scatterplot(x, y, data=df, hue='col', hue_order=['Yes', 'No'], palette=hue_colors)
plt.show()

##### relational plots and subplots #####
relplot() # create subplots in a single figure. Basically replaces scatterplot.

sns.replot(x, y, data=df, kind='scatter', col='asdf', row='asdf') # plots divided among row/col groups.
sns.replot(x, y, data=df, kind='scatter', col='asdf', col_wrap = 2) # number of columns
sns.replot(x, y, data=df, kind='scatter', col='asdf', col_wrap = 2, col_order['col1','col2'])
plt.show()


##### more customization #####
sns.relplot(x,y, data=df, kind='scatter', size='cat', hue='cat')
sns.relplot(x,y, data=df, kind='scatter', hue='cat', style='cat')

sns.relplot(x,y, data=df, kind='scatter', alpha=0.5) # opacity of points


##### lineplots #####
sns.relplot(x,y, data=df, kind='line', style='cat', hue='cat', marker=True) # will differ in style  and color
if you don't want style to be different, use dashes = False

95% confidence intervals are automatic. or we can do ci='sd' to get intervals
Or ci=None

##### categorical plots #####
countplots/barplots/boxplot/Point plots

sns.catplot(x, data=df, kind='count', order=['',''])

sns.catplot(x, y, data=df, kind='bar')

Can make category hoizontal bay doing y instead of x

ci=None

sns.catplot(x,y,data=df, kind='box', order=[], sym='',whis=) # empty string for sym will omit outliers.

whis=2, 2*qir
whis=[5,95] # range from 5th to 95th percentile.


point plots are mean of quantitatiev variable
     * different from lineplot because x-axis is categorical plot.

sns.catplot(x,y,data, hue, kind=point, join=False, esimator=np.median, capsize=) 

join=False removes lines
estimator= changes from mean
capsize= change line thingy at top.
ci=None


##### seaborn styles #####
white is default style.
sns.set_style('whitegrid') # this will add grid
sns.set_style('ticks')
sns.set_style('dark')
sns.set_style('darkgrid')

# diverging palettes, good for 2 things on a spectrum
sns.set_palette() # 'RdBu', 'PRGn', 'RdBu_r', 'PRGn_r'

# sequential palettes, continuous value on a scale
sns.set_palette() # 'Greys', ''Blues'', 'PuRd', 'GnBu'

# or do custom
custom_palette = ['']
sns.set_palette(custom_palette)

# scale of vis
sns.set_context() # paper, notebook, talk, poster
# this will scale the text

# use sns with pandas
can do sns.set() # applies sns default settings to pandas vis

sns.despine(left=True, right=True) # this will get rid of lines around plot


# colors from matplotlib in seaborn
sns.set(color_codes=True)
sns.displot(df['col'],color=g)

# see colorpalette
sns.color_palette('Purples',8) # creating a sequential palette
sns.palplot()

##### titles and labels #####
# if using relplot/catplot
g = sns.catplot(x, y, data, kind)
g.fig.suptitle('title', y= ) # y is location of title. TITLE FOR OVERALL FIGURE
g.fig_titles('This is {col_name}') # this will be for subplots
g.set(xlabel = '', ylabel = '')
plt.xticks(rotation=90)
plt.show()

# if using scatter, etc.
g = sns.boxplot(x,y,data)
g.set_title('title', y)
g.set(xlabel = '', ylabel = '')
plt.xticks(rotation=90)


##### seaborn hist #####
sns.displot(df['col'], kde=True, bins=10, rug=True, fill=True) # kde is kernel density estimate
sns.displot(df['col'], kind='ecdf') # kde is kernel density estimate

# rug is ticks
# fill is shade under kde


##### seaborn regression lines #####
sns.regplot(data= df, x, y, marker=, order=, x_bins=, fitreg=True) # order is order of polynomial

sns.lmplot(data= df, x, y, hue=, col=) # facetgrid shortcut

# hue will categorize by the parameter chosen through colors
# col will put stuff in different graphs by category but in the same figure.
# fireg True will show the regression line

sns.residplot(data=df, x, y, order=)

##### Added granularity: Seaborn with Matplotlib #####
fig, ax = plt.subplots()
sns.histplot(df['col'], ax = ax) # important to have ax=ax
ax.set(xlabel='', ylabel=, xlim=(0,max), ylim=(0, max), title='')

# Create a figure and axes. Then plot the data
fig, ax = plt.subplots()
sns.histplot(df['fmr_1'], ax=ax)

# Customize the labels and limits
ax.set(xlabel="1 Bedroom Fair Market Rent", xlim=(100,1500), title="US Rent")

# Add vertical lines for the median and mean
ax.axvline(x=mean, color='m', label='Median', linestyle='--', linewidth=2)
ax.axvline(x=median, color='b', label='Mean', linestyle='-', linewidth=2)

# Show the legend and plot the data
ax.legend()
plt.show()


##### other categorical plots #####
sns.stripplot(data=df, x,y, jitter=True)

sns.swarmplot(data=df, x,y)

sns.boxplot(data=df,x,y)

sns.violinplot(data=df, x,y)

sns.boxenplot(data=df, x,y)

sns.barplot(data=df, x,y, hue)

sns.pointplot(data=df, x,y, hue)

     sns.countplot(data=df, x,y, hue)

##### matrix plots #####
sns.heatmap(pd.crosstab(df['cat1'], df['cat2']), values=df['value'], agg_func='mean')

arguments:
annot=True, actual values in individual cells
fmt='d', results displayed as integers
cmap='', shading
cbar=True, color bar
linewidths=, spacing between the cells


Correlation matrix visualization

cols =['col1', ..., 'coln']
sns.heatmap(df[cols].corr(), cmap='YlGnBu')


##### FacetGrid #####
g = sns.FacetGrid(df, col='')
g.map(sns.boxplot, 'Tuition', order=[''])


###### PairGrid, pairplot #####
PairGrid shows pairwise relationships between data elements

g = sns.PairGrid(df, vars=['',''])
g = g.map_diag(sns.histplot)
g = g.map_offdiag(sns.scatterplot)
#g = g.map(sns.scatterplot)

pairplot simplifies pairgrid

sns.pairplot(df, vars['',''], kind='reg', diag_kind='hist')
sns.pairplot(df, vars['',''], kind='reg', diag_kind='hist')

sns.pairplot(df, x_vars['',''], y_vars[''] kind='reg', diag_kind='hist')


##### JointGrid #####
it's like a scatterplot, and then a distribution on the axis on the outside of the plot

g = sns.JointGrid(data=df, x, y)
g.plot(sns.regplot, sns.histplot)

2d Contour + distribution
g = sns.JointGrid(data=df, x, y)
g = g.plot_joint(sns.kdeplot)
g = g.plot_marginals(sns.kdeplot, shade=True)

jointplot simplified JointGrid

sns.jointplot(data=df, x, y, kind='')

----------------------------------------------------------------------------------------
Python functions


def func()
     """Docstring: Describe what function does and returns"""
     stuff = (stuff1, stuff2)
     return(stuff)

s1, s2 = func()


glocal scope = defined in main body
local = defined in function

flexible arguments
def func(*args)

def func(*kargs)
     where kwargs is a dictionary

lambdas: write funcs quickly
square_all = map(lambda num: num**2, nums) # element wise **2 from nums list

# filter
result = filter(lambda  x: x[0:2] == 'RT', tweets_df['text'])

add_2 = (lambda num: num + 2)

print(list(square_all))


##### error handling #####

try:
     do stuff
except: # can add TypeError to specify
     if error happens, pring something out

ValueErrors
if echo < 0:
        raise ValueError('echo must be greater than or equal to 0')


----------------------------------------------------------------------------------------
Iterators
iter() # iterable types have iter() method associated with it.

word = 'Berkeley'
it = iter(word)
next(it)
# B
next(it)
# e

print(*it)
# B e r k e l e y


iterating over files
file = open('file.txt')
it = iter(file)
print(next(it)) # prints first line of txt file.

##### enumerate() #####
index, values = enumerate(iterable, begins = 0) # begins indexing at 0

for index, value in enumerate(list):

##### zip() #####

list1 = ['Will', 'Brad']
list2 = ['Smith', 'Pitt']

print(list(zip(list1, list2))) # [('Will', 'Smith'), ('Brad', 'Pitt')]

for l1, l2 in zip(list1, list2):
     print(l1, l2)


##### loading data in chunks #####
result = []
for chunk in pd.read_csv('data.csv', chunksize = 1000):
     result.append(sum(chunk['col']))


##### list comprehensions #####
# collapses for loops for building lists into a single line.
nums = [12, 8, 21, 3, 16]
new_nums = [num + 1 for num in nums] # num is value, nums is list.
print(new_nums) # 13, 9, 22, 4, 17


# filter for only even
[num **2 for num in range(10) if num % 2 == 0]

new_numbers = [num**2 if COND1 else num**3 for num in numbers]


##### dictionary comprehension #####
pos_neg = {num : - num for num in range(0,9)}



##### generators #####
like comprehension but uses () instead of []

Good because if generator is really long, it uses lazy evaluation so it doesn't have to store stuff in memory
same syntax as comprehension

result = (num  for num in range(0, 31))
print(next(result)) # this gets us 0


### generator functions ###

# Create a list of strings
lannister = ['cersei', 'jaime', 'tywin', 'tyrion', 'joffrey']

# Define generator function get_lengths
def get_lengths(input_list):
    """Generator function that yields the
    length of the strings in input_list."""

    # Yield the length of a string
    for person in input_list:
        yield len(person)

# Print the values generated by get_lengths()
for value in get_lengths(lannister):
    print(value)



----------------------------------------------------------------------------------------
General EDA functions

df['col'].value_counts() # get categorical counts
df.info() # datatype, and number of nonmissing values
df.describe() # count, mean, std, etc. of column
df.dtypes # get datatypes

df['col'].str.contains('string') # returns boolean of rows that contain string in column
df['col'].str.contains('string1|string2') # multiple strings

df['col'].astype() # change column datatype

df['col1'].isin(['col1cat1', 'col1cat2']) # true false of is in
~df['col1'].isin(['col1cat1', 'col1cat2']) # invert t/f

df[df['col1'].isin(['col1cat1', 'col1cat2'])] # filter for just in list


df.select_dtypes('datatype') # select columns wth only datatype
df.select_dtypes('object') # preview columns
df.['col'].nunique() # number of unique categories in column

df['col'].min() # get minimum value
df['col'].max() # get the maximum value
sns.boxplot(data,x,y) # where you can break it down by category with y

## group by aggregation
df.groupby('col').mean() # sum, count, min, max, var, std

# .agg() applies aggregating function across a DataFrame
df.agg(['mean', 'std']) # returns a datafram of aggregated columns

# specifying aggregation for columns
df.agg({'col1':['mean','std'], 'col2':['median']})

# named summary columns with specific aggregation
df.groupby('col').agg(mean_rating=('col1', 'mean'), std_rating=('col1', 'std'))

##### missing values stuff #####
df.isna().sum() # number of missing values by column

Strategies for addressing missing data
 * drop missing values
 * impute mean, median, mode
 * impute by sub-group


# dropping columns with 5% missing 
threshold = len(df)*0.05
cols_to_drop = df.columns[df.isna().sum() <= treshold]
df.dropna(subset=cols_to_drop, inplace=True)

# imputing a summary statistic
cols_with_missing_values = df.columns[df.isna().sum() > 0]

for col in cols_with_missing_values[:-1]
     for df[col].fillna(df[col].mode()[0])

# imputing by sub-group
df_dict = df.groupby('group')['value'].median().to_dict()
df['value'] = df['value'].fillna(df['group'].map(df_dict))


# creating new categorical column based on values of another column #
new_category = ['new_cat1', 'new_cat2']

cat1_string = ['cat1s|cond1']
cat2_string = ['cat2s|cond2']

conditions = [(df['orig_col'].str.contains(cat1_string)), df['orig_col'].str.contains(cat2_string))]

df['new_col'] = np.select(conditions, new_category, default='Other') # if not found, it's other


##### data conversion #####
pd.Series.str.replace('chars to remove', 'replacement')# removing commas
df['dollars'] = df['dollars_with_commas'].str.replace(',', '')
df['dollars'] = df['dollars'].astype(float)


Groupby summary statistic, and creating new col in dataframe
df['std_col'] = df.groupby('category')['value'].tranform(lambda x: x.std())

### outliers ###
upper outlier > 75th percentile + (1.5*iqr)
lower outlier < 25th percentile - (1.5*iqr)

# calculating IQR
seventy_fifth = df['col'].quantile(0.75)
twenty_fifth = df['col'].quantile(0.25)
iqr = seventy_fifth - twenty_fifth


# filtering for outliers
lower = twenty_fifth - (1.5*iqr)
upper = seventy_fifth + (1.5*iqr)
no_outliers = df[(df['col'] > lower) & df['col'] < upper]


##### datetime stuff #####
# when loading data in
pd.read_csv('.csv', parse_dates=['cols'])

# when data already loaded in
df['col'] = pd.to_datetime(df['col'])

# when m/d/y are loaded into different columns
df['col'] = pd.to_datetime(df[['m', 'd', 'y']])

# extracting month day or year.
col['date'].dt.month
col['date'].dt.day
col['date'].dt.year

# plotting datetime
sns.lineplot(data=df, x, y) # will plot avg y w.r.t. y with confidence interval.

### correlation stuff ###
df.corr
sns.heatman(df.corr(), annot=True)
sns.pairplot(data=df)
sns.pairplot(data=df, vars=['']) 










