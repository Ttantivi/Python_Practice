{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Navigate to the parent directory\n",
    "data_dir = os.path.abspath(os.path.join(current_dir, os.pardir, 'Pset3', 'data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in data\n",
    "file_path_train = os.path.join(data_dir, \"mnist_train.csv\")\n",
    "file_path_test = os.path.join(data_dir, \"mnist_test.csv\")\n",
    "\n",
    "train_df = pd.read_csv(file_path_train)\n",
    "test_df = pd.read_csv(file_path_test)\n",
    "\n",
    "x_train = train_df.drop('label', axis = 1)\n",
    "y_train = train_df['label']\n",
    "\n",
    "x_test = test_df.drop('label', axis = 1)\n",
    "y_test = test_df['label']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "We need to normalize the data by dividing everything but the label by 255. 255 because we're working with computer vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide all of predictors by 255\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing with PCA for dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the PCA\n",
    "pca = PCA()\n",
    "pca.fit(x_train)\n",
    "\n",
    "# getting total number of principal components\n",
    "num_pcs = x_train.shape[1]\n",
    "\n",
    "# pca.transform returns a numpy array\n",
    "pca_train = pca.transform(x_train)[:, :num_pcs]\n",
    "pca_test = pca.transform(x_test)[:, :num_pcs]\n",
    "\n",
    "# we turn the numpy array into a pandas dataframe\n",
    "pca_train_df = pd.DataFrame(pca_train, columns=[f\"PC{i+1}\" for i in range(num_pcs)])\n",
    "pca_test_df = pd.DataFrame(pca_test, columns=[f\"PC{i+1}\" for i in range(num_pcs)])\n",
    "\n",
    "# Insert the new column at the front\n",
    "#pca_train_df.insert(0, 'label', y_train)\n",
    "#pca_test_df.insert(0, 'label', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87 pcs to explain 90.0 % of variance\n"
     ]
    }
   ],
   "source": [
    "# doing what's explained in above markdown\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "def explain_perc_var(perc_explain):\n",
    "    num_pcs = np.argmax(cumulative_variance_ratio >= perc_explain) + 1\n",
    "    perc_explain = float(perc_explain) * 100\n",
    "    print(num_pcs,'pcs to explain', perc_explain, '% of variance')\n",
    "    return(num_pcs)\n",
    "\n",
    "# getting enough principal components to explain 90% of variance\n",
    "num_pcs_90 = explain_perc_var(0.9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we keep 87 principal components in our matrix now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 88)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# + 1 because we also have to include the label as well.\n",
    "pca_train_df = pca_train_df.iloc[:, 0:num_pcs_90+1]\n",
    "pca_test_df = pca_test_df.iloc[:, 0:num_pcs_90+1]\n",
    "\n",
    "pca_train_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure the code actually works ok, let's do a sample of 1,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 1000 rows from the predictor DataFrame\n",
    "pca_train_df_sample = pca_train_df.sample(n=1000, random_state=254)\n",
    "\n",
    "# Sample the corresponding 1000 rows from the labels DataFrame\n",
    "y_train_sample = y_train.loc[pca_train_df_sample.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'C': 6.1584821106602545, 'gamma': 0.01438449888287663}\n",
      "Best cross-validation score:  0.9329999999999999\n"
     ]
    }
   ],
   "source": [
    "# initializing svm\n",
    "svc = svm.SVC(kernel='rbf')\n",
    "\n",
    "# Create a RepeatedKFold object\n",
    "rkf = RepeatedKFold(n_splits=10, n_repeats=3, random_state=254)\n",
    "\n",
    "# Define a grid of hyperparameters\n",
    "param_grid = {\n",
    "    'C': np.power(10, np.linspace(-5, 5, num=20)),\n",
    "    'gamma': np.power(10, np.linspace(-5, 5, num=20))\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_clf = GridSearchCV(svc, param_grid, cv=rkf, scoring='accuracy', n_jobs=-1)\n",
    "grid_clf.fit(pca_train_df_sample, y_train_sample)\n",
    "\n",
    "# Print the best parameters and the corresponding score\n",
    "print(\"Best parameters: \", grid_clf.best_params_)\n",
    "print(\"Best cross-validation score: \", grid_clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9302\n"
     ]
    }
   ],
   "source": [
    "# Predict on a test set\n",
    "y_pred = grid_clf.predict(pca_test_df)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# printing accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Test accuracy:', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
